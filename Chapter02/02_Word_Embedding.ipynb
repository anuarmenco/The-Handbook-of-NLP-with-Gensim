{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTIQcF4VOuYS"
      },
      "source": [
        "#(2) Word Embedding\n",
        "\n",
        "Teniendo en cuenta que no tengo el archivo de requirements.txt hice ingenieria inversa. Le pregunte a Copilot que según los requerimientos indicados en la [página](https://github.com/PacktPublishing/The-Handbook-of-NLP-with-Gensim/blob/main/Chapter02/02_Word_Embedding.ipynb) cuales libreria debía importar.\n",
        "\n",
        "Entonces, en la carpeta main cree un archivo con el comando \n",
        "`nano requirements.txt` \n",
        "\n",
        "y luego, dentro de nano copié:\n",
        "```\n",
        "gensim==4.3.1\n",
        "nltk==3.8.1\n",
        "numpy==1.23.5\n",
        "scipy==1.10.1\n",
        "session_info==1.0.0\n",
        "sklearn==1.2.2\n",
        "````\n",
        "terminé con ``control+O`` para grabar el archivo y ``control+X`` para cerrar el editor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora instalo el el contenido de `requirements.txt` con"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: anyio==4.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: argon2-cffi==23.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (21.2.0)\n",
            "Requirement already satisfied: arrow==1.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: asttokens==2.4.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 5)) (2.4.1)\n",
            "Requirement already satisfied: async-lru==2.0.4 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 6)) (2.0.4)\n",
            "Requirement already satisfied: attrs==23.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 7)) (23.1.0)\n",
            "Requirement already satisfied: Babel==2.13.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 8)) (2.13.1)\n",
            "Requirement already satisfied: beautifulsoup4==4.12.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 9)) (4.12.2)\n",
            "Requirement already satisfied: bleach==6.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 10)) (6.1.0)\n",
            "Requirement already satisfied: certifi==2023.7.22 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 11)) (2023.7.22)\n",
            "Requirement already satisfied: cffi==1.16.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 12)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer==3.3.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 13)) (3.3.2)\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r ../requirements.txt (line 14)) (8.1.7)\n",
            "Requirement already satisfied: colorama==0.4.6 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 15)) (0.4.6)\n",
            "Requirement already satisfied: comm==0.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 16)) (0.2.0)\n",
            "Requirement already satisfied: contourpy==1.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 17)) (1.2.0)\n",
            "Requirement already satisfied: cycler==0.12.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 18)) (0.12.1)\n",
            "Requirement already satisfied: debugpy==1.8.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 19)) (1.8.0)\n",
            "Requirement already satisfied: decorator==5.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 20)) (5.1.1)\n",
            "Requirement already satisfied: defusedxml==0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 21)) (0.7.1)\n",
            "Requirement already satisfied: exceptiongroup==1.1.3 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 22)) (1.1.3)\n",
            "Requirement already satisfied: executing==2.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 23)) (2.0.1)\n",
            "Requirement already satisfied: fastjsonschema==2.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 24)) (2.19.0)\n",
            "Requirement already satisfied: filelock==3.13.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 25)) (3.13.1)\n",
            "Requirement already satisfied: fonttools==4.44.3 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 26)) (4.44.3)\n",
            "Requirement already satisfied: fqdn==1.5.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 27)) (1.5.1)\n",
            "Requirement already satisfied: fsspec==2023.10.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 28)) (2023.10.0)\n",
            "Requirement already satisfied: gensim==4.3.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r ../requirements.txt (line 29)) (4.3.2)\n",
            "Requirement already satisfied: gitdb==4.0.11 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 30)) (4.0.11)\n",
            "Requirement already satisfied: GitPython==3.1.40 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 31)) (3.1.40)\n",
            "Requirement already satisfied: idna==3.4 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 32)) (3.4)\n",
            "Requirement already satisfied: ipykernel==6.26.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 33)) (6.26.0)\n",
            "Requirement already satisfied: ipython==8.17.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 34)) (8.17.2)\n",
            "Requirement already satisfied: isoduration==20.11.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 35)) (20.11.0)\n",
            "Requirement already satisfied: jedi==0.19.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 36)) (0.19.1)\n",
            "Requirement already satisfied: Jinja2==3.1.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 37)) (3.1.2)\n",
            "Requirement already satisfied: joblib==1.3.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 38)) (1.3.2)\n",
            "Requirement already satisfied: json5==0.9.14 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 39)) (0.9.14)\n",
            "Requirement already satisfied: jsonpointer==2.4 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 40)) (2.4)\n",
            "Requirement already satisfied: jsonschema==4.20.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 41)) (4.20.0)\n",
            "Requirement already satisfied: jsonschema-specifications==2023.11.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 42)) (2023.11.1)\n",
            "Requirement already satisfied: jupyter-events==0.9.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 43)) (0.9.0)\n",
            "Requirement already satisfied: jupyter-lsp==2.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 44)) (2.2.0)\n",
            "Requirement already satisfied: jupyter-server-mathjax==0.2.6 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 45)) (0.2.6)\n",
            "Requirement already satisfied: jupyter_client==8.6.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 46)) (8.6.0)\n",
            "Requirement already satisfied: jupyter_core==5.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 47)) (5.5.0)\n",
            "Requirement already satisfied: jupyter_server==2.10.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 48)) (2.10.1)\n",
            "Requirement already satisfied: jupyter_server_terminals==0.4.4 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 49)) (0.4.4)\n",
            "Requirement already satisfied: jupyterlab==4.0.8 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 50)) (4.0.8)\n",
            "Requirement already satisfied: jupyterlab-pygments==0.2.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 51)) (0.2.2)\n",
            "Requirement already satisfied: jupyterlab_git==0.44.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 52)) (0.44.0)\n",
            "Requirement already satisfied: jupyterlab_server==2.25.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 53)) (2.25.1)\n",
            "Requirement already satisfied: kiwisolver==1.4.5 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 54)) (1.4.5)\n",
            "Requirement already satisfied: MarkupSafe==2.1.3 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 55)) (2.1.3)\n",
            "Requirement already satisfied: matplotlib==3.8.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 56)) (3.8.1)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 57)) (0.1.6)\n",
            "Requirement already satisfied: mistune==3.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 58)) (3.0.2)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 59)) (1.3.0)\n",
            "Requirement already satisfied: nbclient==0.9.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 60)) (0.9.0)\n",
            "Requirement already satisfied: nbconvert==7.11.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 61)) (7.11.0)\n",
            "Requirement already satisfied: nbdime==3.2.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 62)) (3.2.1)\n",
            "Requirement already satisfied: nbformat==5.9.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 63)) (5.9.2)\n",
            "Requirement already satisfied: nest-asyncio==1.5.8 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 64)) (1.5.8)\n",
            "Requirement already satisfied: networkx==3.2.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 65)) (3.2.1)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r ../requirements.txt (line 66)) (3.8.1)\n",
            "Requirement already satisfied: notebook_shim==0.2.3 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 67)) (0.2.3)\n",
            "Requirement already satisfied: numpy==1.26.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 68)) (1.26.2)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 69)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 70)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 71)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 72)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 73)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 74)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 75)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 76)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 77)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 78)) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.3.101 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 79)) (12.3.101)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 80)) (12.1.105)\n",
            "Requirement already satisfied: overrides==7.4.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 81)) (7.4.0)\n",
            "Requirement already satisfied: packaging==23.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 82)) (23.2)\n",
            "Requirement already satisfied: pandas==2.1.3 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 83)) (2.1.3)\n",
            "Requirement already satisfied: pandocfilters==1.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 84)) (1.5.0)\n",
            "Requirement already satisfied: parso==0.8.3 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 85)) (0.8.3)\n",
            "Requirement already satisfied: pexpect==4.8.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 86)) (4.8.0)\n",
            "Requirement already satisfied: Pillow==10.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 87)) (10.1.0)\n",
            "Requirement already satisfied: platformdirs==4.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 88)) (4.0.0)\n",
            "Requirement already satisfied: plotly==5.18.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 89)) (5.18.0)\n",
            "Requirement already satisfied: prometheus-client==0.18.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 90)) (0.18.0)\n",
            "Requirement already satisfied: prompt-toolkit==3.0.41 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 91)) (3.0.41)\n",
            "Requirement already satisfied: psutil==5.9.6 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 92)) (5.9.6)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 93)) (0.7.0)\n",
            "Requirement already satisfied: pure-eval==0.2.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 94)) (0.2.2)\n",
            "Requirement already satisfied: pycparser==2.21 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 95)) (2.21)\n",
            "Requirement already satisfied: Pygments==2.16.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 96)) (2.16.1)\n",
            "Requirement already satisfied: pyparsing==3.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 97)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 98)) (2.8.2)\n",
            "Requirement already satisfied: python-json-logger==2.0.7 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 99)) (2.0.7)\n",
            "Requirement already satisfied: pytz==2023.3.post1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 100)) (2023.3.post1)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 101)) (6.0.1)\n",
            "Requirement already satisfied: pyzmq==25.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 102)) (25.1.1)\n",
            "Requirement already satisfied: referencing==0.31.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 103)) (0.31.0)\n",
            "Requirement already satisfied: regex==2023.10.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r ../requirements.txt (line 104)) (2023.10.3)\n",
            "Requirement already satisfied: requests==2.31.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 105)) (2.31.0)\n",
            "Requirement already satisfied: rfc3339-validator==0.1.4 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 106)) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator==0.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 107)) (0.1.1)\n",
            "Requirement already satisfied: rpds-py==0.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 108)) (0.13.0)\n",
            "Requirement already satisfied: scikit-learn==1.3.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 109)) (1.3.2)\n",
            "Requirement already satisfied: scipy==1.11.3 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 110)) (1.11.3)\n",
            "Requirement already satisfied: seaborn==0.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 111)) (0.13.0)\n",
            "Requirement already satisfied: Send2Trash==1.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 112)) (1.8.2)\n",
            "Requirement already satisfied: session-info==1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r ../requirements.txt (line 113)) (1.0.0)\n",
            "Requirement already satisfied: six==1.16.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 114)) (1.16.0)\n",
            "Requirement already satisfied: smart-open==6.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r ../requirements.txt (line 115)) (6.4.0)\n",
            "Requirement already satisfied: smmap==5.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 116)) (5.0.1)\n",
            "Requirement already satisfied: sniffio==1.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 117)) (1.3.0)\n",
            "Requirement already satisfied: soupsieve==2.5 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 118)) (2.5)\n",
            "Requirement already satisfied: stack-data==0.6.3 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 119)) (0.6.3)\n",
            "Requirement already satisfied: stdlib-list==0.10.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r ../requirements.txt (line 120)) (0.10.0)\n",
            "Requirement already satisfied: sympy==1.12 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 121)) (1.12)\n",
            "Requirement already satisfied: tenacity==8.2.3 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 122)) (8.2.3)\n",
            "Requirement already satisfied: terminado==0.18.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 123)) (0.18.0)\n",
            "Requirement already satisfied: threadpoolctl==3.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 124)) (3.2.0)\n",
            "Requirement already satisfied: tinycss2==1.2.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 125)) (1.2.1)\n",
            "Requirement already satisfied: tomli==2.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 126)) (2.0.1)\n",
            "Requirement already satisfied: torch==2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 127)) (2.1.1)\n",
            "Requirement already satisfied: tornado==6.3.3 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 128)) (6.3.3)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r ../requirements.txt (line 129)) (4.66.1)\n",
            "Requirement already satisfied: traitlets==5.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 130)) (5.13.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 131)) (2.1.0)\n",
            "Requirement already satisfied: types-python-dateutil==2.8.19.14 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 132)) (2.8.19.14)\n",
            "Requirement already satisfied: typing_extensions==4.8.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 133)) (4.8.0)\n",
            "Requirement already satisfied: tzdata==2023.3 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 134)) (2023.3)\n",
            "Requirement already satisfied: uri-template==1.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 135)) (1.3.0)\n",
            "Requirement already satisfied: urllib3==2.0.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r ../requirements.txt (line 136)) (2.0.7)\n",
            "Requirement already satisfied: wcwidth==0.2.10 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 137)) (0.2.10)\n",
            "Requirement already satisfied: webcolors==1.13 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 138)) (1.13)\n",
            "Requirement already satisfied: webencodings==0.5.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 139)) (0.5.1)\n",
            "Requirement already satisfied: websocket-client==1.6.4 in /home/codespace/.local/lib/python3.10/site-packages (from -r ../requirements.txt (line 140)) (1.6.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r ../requirements.txt\n",
        "#!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q3lQ9JdwT3Kc"
      },
      "outputs": [],
      "source": [
        "#!pip freeze > ../requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "s0bIRqIFX8ij",
        "outputId": "a2972f78-cff4-44f8-97eb-7b4d68be3614"
      },
      "outputs": [],
      "source": [
        "#!pip install session-info\n",
        "\n",
        "#import session_info\n",
        "#session_info.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FtcQ7xJRYAaF",
        "outputId": "8224adbf-e167-4980-83f4-275e5ad527c9"
      },
      "outputs": [],
      "source": [
        "#import session_info\n",
        "#session_info.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9G0EW0KOyur"
      },
      "source": [
        "### (A.1) Bag of words with Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mAYcS1ZnK7Wo"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora import Dictionary\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero se trabaja con una lista de cadenas (str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dHVNJnoDN6jR"
      },
      "outputs": [],
      "source": [
        "doc_list = [\n",
        "\"Start spreading the news\",\n",
        "\"You're leaving today (tell him friend)\",\n",
        "\"I want to be a part of it, New York, New York\",\n",
        "\"Your vagabond shoes, they are longing to stray\",\n",
        "\"And steps around the heart of it, New York, New York\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4Aedsr9QEU7",
        "outputId": "13f661d3-63f0-48de-9274-0c85de47fd4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Start spreading the news', \"You're leaving today (tell him friend)\", 'I want to be a part of it, New York, New York', 'Your vagabond shoes, they are longing to stray', 'And steps around the heart of it, New York, New York']\n"
          ]
        }
      ],
      "source": [
        "print(doc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaBWgzd8PuM2",
        "outputId": "c75b3462-50aa-405b-f4c0-fc5a53d380f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   'Start spreading the news',\n",
            "    \"You're leaving today (tell him friend)\",\n",
            "    'I want to be a part of it, New York, New York',\n",
            "    'Your vagabond shoes, they are longing to stray',\n",
            "    'And steps around the heart of it, New York, New York']\n"
          ]
        }
      ],
      "source": [
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(doc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4yh9SzNPG67",
        "outputId": "6289950b-4b44-499e-c31c-d94fc987e7c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['start', 'spreading', 'the', 'news'],\n",
              " ['you', 're', 'leaving', 'today', 'tell', 'him', 'friend'],\n",
              " ['want', 'to', 'be', 'part', 'of', 'it', 'new', 'york', 'new', 'york'],\n",
              " ['your', 'vagabond', 'shoes', 'they', 'are', 'longing', 'to', 'stray'],\n",
              " ['and',\n",
              "  'steps',\n",
              "  'around',\n",
              "  'the',\n",
              "  'heart',\n",
              "  'of',\n",
              "  'it',\n",
              "  'new',\n",
              "  'york',\n",
              "  'new',\n",
              "  'york']]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
        "doc_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eibm93SHcj8x",
        "outputId": "880dd9d6-e090-434b-9485-36d0b5786c03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['start', 'spreading', 'the', 'news'],\n",
              " ['you', 're', 'leaving', 'today', 'tell', 'him', 'friend'],\n",
              " ['want', 'to', 'be', 'part', 'of', 'it', 'new', 'york', 'new', 'york'],\n",
              " ['your', 'vagabond', 'shoes', 'they', 'are', 'longing', 'to', 'stray'],\n",
              " ['and',\n",
              "  'steps',\n",
              "  'around',\n",
              "  'the',\n",
              "  'heart',\n",
              "  'of',\n",
              "  'it',\n",
              "  'new',\n",
              "  'york',\n",
              "  'new',\n",
              "  'york']]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_tokenized = []\n",
        "for doc in doc_list:\n",
        "    doc_tokenized.append(simple_preprocess(doc))\n",
        "doc_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qddBm-gyPMka",
        "outputId": "6feea42d-c087-472b-bc59-647ae742755a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<gensim.corpora.dictionary.Dictionary at 0x7f94f044db40>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dictionary = Dictionary()\n",
        "dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJkvW-ojPReH",
        "outputId": "254c8b97-99b3-4d71-9220-333d7855c0b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
              " [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
              " [(11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2)],\n",
              " [(16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
              " [(3, 1),\n",
              "  (12, 1),\n",
              "  (13, 2),\n",
              "  (14, 1),\n",
              "  (18, 2),\n",
              "  (26, 1),\n",
              "  (27, 1),\n",
              "  (28, 1),\n",
              "  (29, 1)]]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
        "BoW_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLfJcvwDPalO",
        "outputId": "f70bd353-23fd-4d11-a843-7ef8212da202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)], [(11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2)], [(16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(3, 1), (12, 1), (13, 2), (14, 1), (18, 2), (26, 1), (27, 1), (28, 1), (29, 1)]]\n"
          ]
        }
      ],
      "source": [
        "print(BoW_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQzNHVcCPePM",
        "outputId": "487d141d-b64f-40e9-a19c-2453a2926e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[('news', 1), ('spreading', 1), ('start', 1), ('the', 1)], [('friend', 1), ('him', 1), ('leaving', 1), ('re', 1), ('tell', 1), ('today', 1), ('you', 1)], [('be', 1), ('it', 1), ('new', 2), ('of', 1), ('part', 1), ('to', 1), ('want', 1), ('york', 2)], [('to', 1), ('are', 1), ('longing', 1), ('shoes', 1), ('stray', 1), ('they', 1), ('vagabond', 1), ('your', 1)], [('the', 1), ('it', 1), ('new', 2), ('of', 1), ('york', 2), ('and', 1), ('around', 1), ('heart', 1), ('steps', 1)]]\n"
          ]
        }
      ],
      "source": [
        "id_words = [[(dictionary[id], count) for id, count in line] for line in BoW_corpus]\n",
        "print(id_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora se realiza el procedimiento anterior, pero no con una lista de cadenas por cada frase o sentencia, sino una sola cadena que es todo el documento.\n",
        "De este modo dará un resultado diferente, con estos 2 procesos se diferencias los valores por cada frase y del documentos completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxZ78B-mUWiQ",
        "outputId": "c188f53e-38da-4aca-e4f6-5867235f863e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['start',\n",
              " 'spreading',\n",
              " 'the',\n",
              " 'news',\n",
              " 'you',\n",
              " 're',\n",
              " 'leaving',\n",
              " 'today',\n",
              " 'tell',\n",
              " 'him',\n",
              " 'friend',\n",
              " 'want',\n",
              " 'to',\n",
              " 'be',\n",
              " 'part',\n",
              " 'of',\n",
              " 'it',\n",
              " 'new',\n",
              " 'york',\n",
              " 'new',\n",
              " 'york',\n",
              " 'your',\n",
              " 'vagabond',\n",
              " 'shoes',\n",
              " 'they',\n",
              " 'are',\n",
              " 'longing',\n",
              " 'to',\n",
              " 'stray',\n",
              " 'and',\n",
              " 'steps',\n",
              " 'around',\n",
              " 'the',\n",
              " 'heart',\n",
              " 'of',\n",
              " 'it',\n",
              " 'new',\n",
              " 'york',\n",
              " 'new',\n",
              " 'york']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_list_long = \"Start spreading the news, You're leaving today (tell him friend), I want to be a part of it, New York, New York, Your vagabond shoes, they are longing to stray, And steps around the heart of it, New York, New York\"\n",
        "doc_list_tokenized_long = simple_preprocess(doc_list_long)\n",
        "doc_list_tokenized_long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26CywD9YjilJ",
        "outputId": "aea9c5fb-26f3-47e2-c935-4d6274f578b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 1),\n",
              " (1, 1),\n",
              " (2, 1),\n",
              " (3, 2),\n",
              " (4, 1),\n",
              " (5, 1),\n",
              " (6, 1),\n",
              " (7, 1),\n",
              " (8, 1),\n",
              " (9, 1),\n",
              " (10, 1),\n",
              " (11, 1),\n",
              " (12, 2),\n",
              " (13, 4),\n",
              " (14, 2),\n",
              " (15, 1),\n",
              " (16, 2),\n",
              " (17, 1),\n",
              " (18, 4),\n",
              " (19, 1),\n",
              " (20, 1),\n",
              " (21, 1),\n",
              " (22, 1),\n",
              " (23, 1),\n",
              " (24, 1),\n",
              " (25, 1),\n",
              " (26, 1),\n",
              " (27, 1),\n",
              " (28, 1),\n",
              " (29, 1)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BoW_corpus_long = dictionary.doc2bow(doc_list_tokenized_long)\n",
        "BoW_corpus_long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bYm2JpgkeIn",
        "outputId": "b754e685-d890-4d0d-f1f8-d22651d9c351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   ('news', 1),\n",
            "    ('spreading', 1),\n",
            "    ('start', 1),\n",
            "    ('the', 2),\n",
            "    ('friend', 1),\n",
            "    ('him', 1),\n",
            "    ('leaving', 1),\n",
            "    ('re', 1),\n",
            "    ('tell', 1),\n",
            "    ('today', 1),\n",
            "    ('you', 1),\n",
            "    ('be', 1),\n",
            "    ('it', 2),\n",
            "    ('new', 4),\n",
            "    ('of', 2),\n",
            "    ('part', 1),\n",
            "    ('to', 2),\n",
            "    ('want', 1),\n",
            "    ('york', 4),\n",
            "    ('are', 1),\n",
            "    ('longing', 1),\n",
            "    ('shoes', 1),\n",
            "    ('stray', 1),\n",
            "    ('they', 1),\n",
            "    ('vagabond', 1),\n",
            "    ('your', 1),\n",
            "    ('and', 1),\n",
            "    ('around', 1),\n",
            "    ('heart', 1),\n",
            "    ('steps', 1)]\n"
          ]
        }
      ],
      "source": [
        "id_words_long = [(dictionary[id], count) for id, count in BoW_corpus_long]\n",
        "pp.pprint(id_words_long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M0XsyhkQbcM"
      },
      "source": [
        "### (A.2) Word of Bags with scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "c2i_fN0TndA8"
      },
      "outputs": [],
      "source": [
        "doc_list = [\n",
        "\"Start spreading the news\",\n",
        "\"You're leaving today (tell him friend)\",\n",
        "\"I want to be a part of it, New York, New York\",\n",
        "\"Your vagabond shoes, they are longing to stray\",\n",
        "\"And steps around the heart of it, New York, New York\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj4AbY9eqS5q",
        "outputId": "5b2b15e6-2983-48f8-ed4b-b97a91aca7b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{   'and': 1,\n",
            "    'are': 1,\n",
            "    'around': 1,\n",
            "    'be': 1,\n",
            "    'friend': 1,\n",
            "    'heart': 1,\n",
            "    'him': 1,\n",
            "    'it': 2,\n",
            "    'leaving': 1,\n",
            "    'longing': 1,\n",
            "    'new': 4,\n",
            "    'news': 1,\n",
            "    'of': 2,\n",
            "    'part': 1,\n",
            "    're': 1,\n",
            "    'shoes': 1,\n",
            "    'spreading': 1,\n",
            "    'start': 1,\n",
            "    'steps': 1,\n",
            "    'stray': 1,\n",
            "    'tell': 1,\n",
            "    'the': 2,\n",
            "    'they': 1,\n",
            "    'to': 2,\n",
            "    'today': 1,\n",
            "    'vagabond': 1,\n",
            "    'want': 1,\n",
            "    'york': 4,\n",
            "    'you': 1,\n",
            "    'your': 1}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "cv_fit = cv.fit_transform(doc_list)\n",
        "word_list = cv.get_feature_names_out()\n",
        "count_list = cv_fit.toarray().sum(axis=0)\n",
        "pp.pprint( dict(zip(word_list,count_list)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0r6oyMXs4h4",
        "outputId": "0c9bc51f-c90c-426a-8e6e-e1970ffc5eb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['and', 'are', 'around', 'be', 'friend', 'heart', 'him', 'it',\n",
              "       'leaving', 'longing', 'new', 'news', 'of', 'part', 're', 'shoes',\n",
              "       'spreading', 'start', 'steps', 'stray', 'tell', 'the', 'they',\n",
              "       'to', 'today', 'vagabond', 'want', 'york', 'you', 'your'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hacPhRhsNSI",
        "outputId": "e8b9c250-21d1-46c5-9ba4-138ada5eaf73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
              "        0, 0, 1, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 0, 0, 1, 2, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "        1, 1, 0, 1, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "        0, 0, 0, 0, 0, 2, 0, 0]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv_fit.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HbfilJoGQbDi"
      },
      "outputs": [],
      "source": [
        "count_list = cv_fit.toarray().sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{   'and': 1,\n",
            "    'are': 1,\n",
            "    'around': 1,\n",
            "    'be': 1,\n",
            "    'friend': 1,\n",
            "    'heart': 1,\n",
            "    'him': 1,\n",
            "    'it': 2,\n",
            "    'leaving': 1,\n",
            "    'longing': 1,\n",
            "    'new': 4,\n",
            "    'news': 1,\n",
            "    'of': 2,\n",
            "    'part': 1,\n",
            "    're': 1,\n",
            "    'shoes': 1,\n",
            "    'spreading': 1,\n",
            "    'start': 1,\n",
            "    'steps': 1,\n",
            "    'stray': 1,\n",
            "    'tell': 1,\n",
            "    'the': 2,\n",
            "    'they': 1,\n",
            "    'to': 2,\n",
            "    'today': 1,\n",
            "    'vagabond': 1,\n",
            "    'want': 1,\n",
            "    'york': 4,\n",
            "    'you': 1,\n",
            "    'your': 1}\n"
          ]
        }
      ],
      "source": [
        "pp.pprint( dict(zip(word_list,count_list)) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yen15jcASwMa"
      },
      "source": [
        "### (B.1)  Bag of N-grams with Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApD5Yunp2229",
        "outputId": "d33e6e5c-578a-4707-c170-a0282e87dc24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['start', 'spreading', 'the', 'news'],\n",
              " ['you', 're', 'leaving', 'today'],\n",
              " ['want', 'to', 'be', 'part', 'of', 'it', 'new', 'york', 'new', 'york'],\n",
              " ['your', 'vagabond', 'shoes', 'they', 'are', 'longing', 'to', 'stray'],\n",
              " ['and',\n",
              "  'steps',\n",
              "  'around',\n",
              "  'the',\n",
              "  'heart',\n",
              "  'of',\n",
              "  'it',\n",
              "  'new',\n",
              "  'york',\n",
              "  'new',\n",
              "  'york'],\n",
              " ['come', 'and', 'visit', 'us'],\n",
              " ['come', 'and', 'visit', 'the', 'city']]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_list_Ngrams = [\n",
        "\"Start spreading the news\",\n",
        "\"You're leaving today\",\n",
        "\"I want to be a part of it, New York, New York\",\n",
        "\"Your vagabond shoes, they are longing to stray\",\n",
        "\"And steps around the heart of it, New York, New York\",\n",
        "\"Come and visit us\",\n",
        "\"Come and visit the city\",\n",
        "]\n",
        "doc_tokenized_Ngrams = [simple_preprocess(doc) for doc in doc_list_Ngrams]\n",
        "doc_tokenized_Ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "#type(doc_tokenized_Ngrams[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cF6aLbHzfwz",
        "outputId": "38d89bdc-2273-4a58-fe48-c68887d5cacf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
              " [(4, 1), (5, 1), (6, 1), (7, 1)],\n",
              " [(8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2)],\n",
              " [(13, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)],\n",
              " [(3, 1),\n",
              "  (9, 1),\n",
              "  (10, 2),\n",
              "  (11, 1),\n",
              "  (15, 2),\n",
              "  (23, 1),\n",
              "  (24, 1),\n",
              "  (25, 1),\n",
              "  (26, 1)],\n",
              " [(23, 1), (27, 1), (28, 1), (29, 1)],\n",
              " [(3, 1), (23, 1), (27, 1), (29, 1), (30, 1)]]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.models.phrases import Phrases\n",
        "mydict = Dictionary()\n",
        "mycorpus_Ngrams = [mydict.doc2bow(doc, allow_update=True) for doc in doc_tokenized_Ngrams]\n",
        "mycorpus_Ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m58sd7QelIi",
        "outputId": "088fd792-832d-4f31-9330-69f5ebe7faa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FrozenPhrases<6 phrases, min_count=1, threshold=2>\n",
            "['start', 'spreading', 'the', 'news']\n",
            "['you', 're', 'leaving', 'today']\n",
            "['want', 'to', 'be', 'part', 'of it', 'new york', 'new york']\n",
            "['your', 'vagabond', 'shoes', 'they', 'are', 'longing', 'to', 'stray']\n",
            "['and', 'steps', 'around', 'the', 'heart', 'of it', 'new york', 'new york']\n",
            "['come and', 'visit', 'us']\n",
            "['come and', 'visit', 'the', 'city']\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "\n",
        "# Build the bigram models\n",
        "bigram = gensim.models.phrases.Phrases(doc_tokenized_Ngrams, min_count=3, threshold=10)\n",
        "bigram = Phrases(doc_tokenized_Ngrams, min_count=1, threshold=2, delimiter=' ')\n",
        "bigram_phraser = Phraser(bigram)\n",
        "print(bigram_phraser)\n",
        "\n",
        "for sent in doc_tokenized_Ngrams:\n",
        "    tokens_ = bigram_phraser[sent]\n",
        "    print(tokens_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mSfnVtpelSw",
        "outputId": "778c0d5e-748e-41dc-e4b0-e00d45f6ea69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['come and visit']\n",
            "['come and visit']\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Phrases\n",
        "\n",
        "bigram = Phrases(doc_tokenized_Ngrams, min_count=1, delimiter=' ')\n",
        "trigram = Phrases(bigram[doc_tokenized_Ngrams], min_count=1, delimiter=' ')\n",
        "\n",
        "for sent in doc_tokenized_Ngrams:\n",
        "    bigrams_ = [b for b in bigram[sent] if b.count(' ') == 1]\n",
        "    trigrams_ = [t for t in trigram[bigram[sent]] if t.count(' ') == 2]\n",
        "\n",
        "    print(trigrams_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZAwwEXUS9lA"
      },
      "source": [
        "### (B.2) Bag of N-grams with scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_ILS7H_QKwX5"
      },
      "outputs": [],
      "source": [
        "doc_list = [\n",
        "\"Start spreading the news\",\n",
        "\"You're leaving today (tell him friend)\",\n",
        "\"I want to be a part of it, New York, New York\",\n",
        "\"Your vagabond shoes, they are longing to stray\",\n",
        "\"And steps around the heart of it, New York, New York\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "C2EuLGz_Su0U",
        "outputId": "286f05c5-cd1d-4d40-b88c-7557abbd7609"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(ngram_range=(2, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(2, 2))</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "CountVectorizer(ngram_range=(2, 2))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# look at sequences of tokens of minimum length 2 and maximum length 2\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "bigram_vectorizer.fit(doc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19rCFc6V33mD",
        "outputId": "0cae72d8-8fb4-4e9b-f669-18a1d0eac2ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{   'and steps': 1,\n",
            "    'are longing': 1,\n",
            "    'around the': 1,\n",
            "    'be part': 1,\n",
            "    'heart of': 1,\n",
            "    'him friend': 1,\n",
            "    'it new': 2,\n",
            "    'leaving today': 1,\n",
            "    'longing to': 1,\n",
            "    'new york': 4,\n",
            "    'of it': 2,\n",
            "    'part of': 1,\n",
            "    're leaving': 1,\n",
            "    'shoes they': 1,\n",
            "    'spreading the': 1,\n",
            "    'start spreading': 1,\n",
            "    'steps around': 1,\n",
            "    'tell him': 1,\n",
            "    'the heart': 1,\n",
            "    'the news': 1,\n",
            "    'they are': 1,\n",
            "    'to be': 1,\n",
            "    'to stray': 1,\n",
            "    'today tell': 1,\n",
            "    'vagabond shoes': 1,\n",
            "    'want to': 1,\n",
            "    'york new': 2,\n",
            "    'you re': 1,\n",
            "    'your vagabond': 1}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "ngram_fit = ngram_vectorizer.fit_transform(doc_list)\n",
        "word_list = ngram_vectorizer.get_feature_names_out()\n",
        "count_list = ngram_fit.toarray().sum(axis=0)\n",
        "pp.pprint( dict(zip(word_list,count_list)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDCgXZ-N33pC",
        "outputId": "70475fbb-853c-4f2a-d023-7a62ad795784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{   'and steps around': 1,\n",
            "    'are longing to': 1,\n",
            "    'around the heart': 1,\n",
            "    'be part of': 1,\n",
            "    'heart of it': 1,\n",
            "    'it new york': 2,\n",
            "    'leaving today tell': 1,\n",
            "    'longing to stray': 1,\n",
            "    'new york new': 2,\n",
            "    'of it new': 2,\n",
            "    'part of it': 1,\n",
            "    're leaving today': 1,\n",
            "    'shoes they are': 1,\n",
            "    'spreading the news': 1,\n",
            "    'start spreading the': 1,\n",
            "    'steps around the': 1,\n",
            "    'tell him friend': 1,\n",
            "    'the heart of': 1,\n",
            "    'they are longing': 1,\n",
            "    'to be part': 1,\n",
            "    'today tell him': 1,\n",
            "    'vagabond shoes they': 1,\n",
            "    'want to be': 1,\n",
            "    'york new york': 2,\n",
            "    'you re leaving': 1,\n",
            "    'your vagabond shoes': 1}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
        "ngram_fit = ngram_vectorizer.fit_transform(doc_list)\n",
        "word_list = ngram_vectorizer.get_feature_names_out()\n",
        "count_list = ngram_fit.toarray().sum(axis=0)\n",
        "pp.pprint( dict(zip(word_list,count_list)) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr6VzPUpe-oP"
      },
      "source": [
        "### (B.4) Bag of N-grams with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saeAydt_6aWG",
        "outputId": "bb13b1ff-46fb-4096-d068-082bb1d8e952"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['start',\n",
              " 'spreading',\n",
              " 'the',\n",
              " 'news',\n",
              " 'you',\n",
              " 're',\n",
              " 'leaving',\n",
              " 'today',\n",
              " 'tell',\n",
              " 'him',\n",
              " 'friend',\n",
              " 'want',\n",
              " 'to',\n",
              " 'be',\n",
              " 'part',\n",
              " 'of',\n",
              " 'it',\n",
              " 'new',\n",
              " 'york',\n",
              " 'new',\n",
              " 'york',\n",
              " 'your',\n",
              " 'vagabond',\n",
              " 'shoes',\n",
              " 'they',\n",
              " 'are',\n",
              " 'longing',\n",
              " 'to',\n",
              " 'stray',\n",
              " 'and',\n",
              " 'steps',\n",
              " 'around',\n",
              " 'the',\n",
              " 'heart',\n",
              " 'of',\n",
              " 'it',\n",
              " 'new',\n",
              " 'york',\n",
              " 'new',\n",
              " 'york']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.util import bigrams, trigrams\n",
        "flat_list = []\n",
        "for sublist in doc_tokenized:\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)\n",
        "flat_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXLXlO6IRAIH",
        "outputId": "5c1cf5f7-f74a-4a7c-8854-872d976dbd19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('start', 'spreading'),\n",
              " ('spreading', 'the'),\n",
              " ('the', 'news'),\n",
              " ('news', 'you'),\n",
              " ('you', 're'),\n",
              " ('re', 'leaving'),\n",
              " ('leaving', 'today'),\n",
              " ('today', 'tell'),\n",
              " ('tell', 'him'),\n",
              " ('him', 'friend'),\n",
              " ('friend', 'want'),\n",
              " ('want', 'to'),\n",
              " ('to', 'be'),\n",
              " ('be', 'part'),\n",
              " ('part', 'of'),\n",
              " ('of', 'it'),\n",
              " ('it', 'new'),\n",
              " ('new', 'york'),\n",
              " ('york', 'new'),\n",
              " ('new', 'york'),\n",
              " ('york', 'your'),\n",
              " ('your', 'vagabond'),\n",
              " ('vagabond', 'shoes'),\n",
              " ('shoes', 'they'),\n",
              " ('they', 'are'),\n",
              " ('are', 'longing'),\n",
              " ('longing', 'to'),\n",
              " ('to', 'stray'),\n",
              " ('stray', 'and'),\n",
              " ('and', 'steps'),\n",
              " ('steps', 'around'),\n",
              " ('around', 'the'),\n",
              " ('the', 'heart'),\n",
              " ('heart', 'of'),\n",
              " ('of', 'it'),\n",
              " ('it', 'new'),\n",
              " ('new', 'york'),\n",
              " ('york', 'new'),\n",
              " ('new', 'york')]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.util import bigrams, trigrams\n",
        "flat_list = []\n",
        "for sublist in doc_tokenized:\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)\n",
        "\n",
        "nltk_bigrams = list(bigrams(flat_list))\n",
        "nltk_bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBhEEfMWe2cv",
        "outputId": "87e8a388-a302-4d3a-e459-90393ce7496e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('start', 'spreading', 'the'),\n",
              " ('spreading', 'the', 'news'),\n",
              " ('the', 'news', 'you'),\n",
              " ('news', 'you', 're'),\n",
              " ('you', 're', 'leaving'),\n",
              " ('re', 'leaving', 'today'),\n",
              " ('leaving', 'today', 'tell'),\n",
              " ('today', 'tell', 'him'),\n",
              " ('tell', 'him', 'friend'),\n",
              " ('him', 'friend', 'want'),\n",
              " ('friend', 'want', 'to'),\n",
              " ('want', 'to', 'be'),\n",
              " ('to', 'be', 'part'),\n",
              " ('be', 'part', 'of'),\n",
              " ('part', 'of', 'it'),\n",
              " ('of', 'it', 'new'),\n",
              " ('it', 'new', 'york'),\n",
              " ('new', 'york', 'new'),\n",
              " ('york', 'new', 'york'),\n",
              " ('new', 'york', 'your'),\n",
              " ('york', 'your', 'vagabond'),\n",
              " ('your', 'vagabond', 'shoes'),\n",
              " ('vagabond', 'shoes', 'they'),\n",
              " ('shoes', 'they', 'are'),\n",
              " ('they', 'are', 'longing'),\n",
              " ('are', 'longing', 'to'),\n",
              " ('longing', 'to', 'stray'),\n",
              " ('to', 'stray', 'and'),\n",
              " ('stray', 'and', 'steps'),\n",
              " ('and', 'steps', 'around'),\n",
              " ('steps', 'around', 'the'),\n",
              " ('around', 'the', 'heart'),\n",
              " ('the', 'heart', 'of'),\n",
              " ('heart', 'of', 'it'),\n",
              " ('of', 'it', 'new'),\n",
              " ('it', 'new', 'york'),\n",
              " ('new', 'york', 'new'),\n",
              " ('york', 'new', 'york')]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk_trigrams = list(trigrams(flat_list))\n",
        "nltk_trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zyhg-FTve2f1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8MYrn4He2jN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0pA-qC1Rer9"
      },
      "source": [
        "### (C.1) TF-IDF with Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2bIKGLHARhfE"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import pprint\n",
        "from gensim.models import TfidfModel\n",
        "from gensim import corpora\n",
        "from gensim.utils import simple_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
              " [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
              " [(11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2)],\n",
              " [(16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
              " [(3, 1),\n",
              "  (12, 1),\n",
              "  (13, 2),\n",
              "  (14, 1),\n",
              "  (18, 2),\n",
              "  (26, 1),\n",
              "  (27, 1),\n",
              "  (28, 1),\n",
              "  (29, 1)]]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BoW_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEnBJVubsK7E",
        "outputId": "1022b6f4-9f2b-41a2-f025-e5e89499853a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['news', 1], ['spreading', 1], ['start', 1], ['the', 1]]\n",
            "[['friend', 1], ['him', 1], ['leaving', 1], ['re', 1], ['tell', 1], ['today', 1], ['you', 1]]\n",
            "[['be', 1], ['it', 1], ['new', 2], ['of', 1], ['part', 1], ['to', 1], ['want', 1], ['york', 2]]\n",
            "[['to', 1], ['are', 1], ['longing', 1], ['shoes', 1], ['stray', 1], ['they', 1], ['vagabond', 1], ['your', 1]]\n",
            "[['the', 1], ['it', 1], ['new', 2], ['of', 1], ['york', 2], ['and', 1], ['around', 1], ['heart', 1], ['steps', 1]]\n"
          ]
        }
      ],
      "source": [
        "doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
        "dictionary = corpora.Dictionary()\n",
        "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
        "for doc in BoW_corpus:\n",
        "   print([[dictionary[id], freq] for id, freq in doc])\n",
        "import numpy as np\n",
        "tfidf = TfidfModel(BoW_corpus, smartirs='ntc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mType:\u001b[0m           Dictionary\n",
            "\u001b[0;31mString form:\u001b[0m    Dictionary<30 unique tokens: ['news', 'spreading', 'start', 'the', 'friend']...>\n",
            "\u001b[0;31mLength:\u001b[0m         30\n",
            "\u001b[0;31mFile:\u001b[0m           ~/.python/current/lib/python3.10/site-packages/gensim/corpora/dictionary.py\n",
            "\u001b[0;31mDocstring:\u001b[0m     \n",
            "Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
            "\n",
            "Notable instance attributes:\n",
            "\n",
            "Attributes\n",
            "----------\n",
            "token2id : dict of (str, int)\n",
            "    token -> token_id. I.e. the reverse mapping to `self[token_id]`.\n",
            "cfs : dict of (int, int)\n",
            "    Collection frequencies: token_id -> how many instances of this token are contained in the documents.\n",
            "dfs : dict of (int, int)\n",
            "    Document frequencies: token_id -> how many documents contain this token.\n",
            "num_docs : int\n",
            "    Number of documents processed.\n",
            "num_pos : int\n",
            "    Total number of corpus positions (number of processed words).\n",
            "num_nnz : int\n",
            "    Total number of non-zeroes in the BOW matrix (sum of the number of unique\n",
            "    words per document over the entire corpus).\n",
            "\u001b[0;31mInit docstring:\u001b[0m\n",
            "Parameters\n",
            "----------\n",
            "documents : iterable of iterable of str, optional\n",
            "    Documents to be used to initialize the mapping and collect corpus statistics.\n",
            "prune_at : int, optional\n",
            "    Dictionary will try to keep no more than `prune_at` words in its mapping, to limit its RAM\n",
            "    footprint, the correctness is not guaranteed.\n",
            "    Use :meth:`~gensim.corpora.dictionary.Dictionary.filter_extremes` to perform proper filtering.\n",
            "\n",
            "Examples\n",
            "--------\n",
            ".. sourcecode:: pycon\n",
            "\n",
            "    >>> from gensim.corpora import Dictionary\n",
            "    >>>\n",
            "    >>> texts = [['human', 'interface', 'computer']]\n",
            "    >>> dct = Dictionary(texts)  # initialize a Dictionary\n",
            "    >>> dct.add_documents([[\"cat\", \"say\", \"meow\"], [\"dog\"]])  # add more document (extend the vocabulary)\n",
            "    >>> dct.doc2bow([\"dog\", \"computer\", \"non_existent_word\"])\n",
            "    [(0, 1), (6, 1)]"
          ]
        }
      ],
      "source": [
        "dictionary?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ValuesView(<gensim.corpora.dictionary.Dictionary object at 0x7f94bcdcbb80>)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dictionary.itervalues()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['news',\n",
              " 'spreading',\n",
              " 'start',\n",
              " 'the',\n",
              " 'friend',\n",
              " 'him',\n",
              " 'leaving',\n",
              " 're',\n",
              " 'tell',\n",
              " 'today',\n",
              " 'you',\n",
              " 'be',\n",
              " 'it',\n",
              " 'new',\n",
              " 'of',\n",
              " 'part',\n",
              " 'to',\n",
              " 'want',\n",
              " 'york',\n",
              " 'are',\n",
              " 'longing',\n",
              " 'shoes',\n",
              " 'stray',\n",
              " 'they',\n",
              " 'vagabond',\n",
              " 'your',\n",
              " 'and',\n",
              " 'around',\n",
              " 'heart',\n",
              " 'steps']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[valor for valor in dictionary.itervalues()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bexFvcjL_Hay",
        "outputId": "0bd2c265-b53a-4cea-ce0a-faf6744a71b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
              " [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
              " [(11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2)],\n",
              " [(16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
              " [(3, 1),\n",
              "  (12, 1),\n",
              "  (13, 2),\n",
              "  (14, 1),\n",
              "  (18, 2),\n",
              "  (26, 1),\n",
              "  (27, 1),\n",
              "  (28, 1),\n",
              "  (29, 1)]]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BoW_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG8EVVIosetO",
        "outputId": "bb0e2f90-e402-449c-ffe2-bf7e9b160909"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(3, 0.21496814396163463),\n",
              " (12, 0.21496814396163463),\n",
              " (13, 0.42993628792326927),\n",
              " (14, 0.21496814396163463),\n",
              " (18, 0.42993628792326927),\n",
              " (26, 0.35059794205706235),\n",
              " (27, 0.35059794205706235),\n",
              " (28, 0.35059794205706235),\n",
              " (29, 0.35059794205706235)]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the tfidf transformed corpus,\n",
        "# then the vector of the third sentence.\n",
        "tfidf[BoW_corpus][4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eKdB5SqSN4l"
      },
      "source": [
        "### (C.2) TD-IDF with Scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "h1KXMUQ8SR8e",
        "outputId": "decda684-9e96-4f06-b9a2-19978c9e11de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "TfidfVectorizer()"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(doc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9A4sAANST5-",
        "outputId": "30b5d5d0-ba5a-40bd-b12a-f36f07022284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.52 0.   0.\n",
            "  0.   0.   0.52 0.52 0.   0.   0.   0.42 0.   0.   0.   0.   0.   0.\n",
            "  0.   0.  ]\n",
            " [0.   0.   0.   0.   0.38 0.   0.38 0.   0.38 0.   0.   0.   0.   0.\n",
            "  0.38 0.   0.   0.   0.   0.   0.38 0.   0.   0.   0.38 0.   0.   0.\n",
            "  0.38 0.  ]\n",
            " [0.   0.   0.   0.31 0.   0.   0.   0.25 0.   0.   0.51 0.   0.25 0.31\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.31 0.51\n",
            "  0.   0.  ]\n",
            " [0.   0.36 0.   0.   0.   0.   0.   0.   0.   0.36 0.   0.   0.   0.\n",
            "  0.   0.36 0.   0.   0.   0.36 0.   0.   0.36 0.29 0.   0.36 0.   0.\n",
            "  0.   0.36]\n",
            " [0.3  0.   0.3  0.   0.   0.3  0.   0.24 0.   0.   0.48 0.   0.24 0.\n",
            "  0.   0.   0.   0.   0.3  0.   0.   0.24 0.   0.   0.   0.   0.   0.48\n",
            "  0.   0.  ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "print(tfidf_vectorizer.transform(doc_list).toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNyCAHQPSWHT",
        "outputId": "220d228d-27d3-46ca-d2e5-1b5a3ca28823"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['and', 'are', 'around', 'be', 'friend', 'heart', 'him', 'it',\n",
              "       'leaving', 'longing', 'new', 'news', 'of', 'part', 're', 'shoes',\n",
              "       'spreading', 'start', 'steps', 'stray', 'tell', 'the', 'they',\n",
              "       'to', 'today', 'vagabond', 'want', 'york', 'you', 'your'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_T8sDxTSYGi",
        "outputId": "dde41188-2f4f-4b75-cd8f-f244e228f50b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.3 , 0.  , 0.3 , 0.  , 0.  , 0.3 , 0.  , 0.24, 0.  , 0.  , 0.48,\n",
              "       0.  , 0.24, 0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.  , 0.24,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.48, 0.  , 0.  ])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_vectorizer.transform(doc_list).toarray()[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCPTydBAC3Zy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
